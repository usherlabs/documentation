---
sidebar_position: 2
title: 'Primer'
---

## The Purpose

Custom data typically resides in centralized and siloed environments and is requested via centralized APIs.
This exposes security risks associated with data integrity and data availability.
Even when decentralized storage networks are utilised, centralized APIs that gateway data can remain points of compromise.
The team behind the Log Store experienced this issue first-hand whereby using custom data to manage the allocation of digital assets required trust in the entity managing the data.

The Log Store Network solves this problem by decentralizing data management from the point of data capture to storage and then to query.
The technology cryptographically secures data movement from any device to the [Arweave Blockchain](https://www.arweave.org/) network - enabling a pure data store for events indexed and queryable by time.
Log Store's time-series composability ensures systems can track which data they've ingested over time. This streamlines the process to trustless decentralized ETL.
Oracle Networks, Smart Contracts, and other on-chain applications can simply use the last timestamp when data was ingested, to determine which new events to ingest.

By leveraging the simplicity of data transport over the [Streamr Network](https://streamr.network/), consensus-driven data validity facilitated by the [KYVE Network](https://www.kyve.network/) and guaranteed permanence and immutability of data stored by the [Arweave Blockchain](https://www.arweave.org/), the Log Store Network achieves its purpose of decentralizing data management.

To ensure these disparate networks interoperate to deliver this solution, custom integration software is embedded into Nodes that are participating in the KYVE and Streamr Networks. New Smart Contracts ensure the solution is managed through token incentivization. The outcome comprises two sub-networks bound by their own Blockchain systems, and interlinked via a Streamr stream that behaves as a communication mesh.

## A Guide on the Architecture

import LoomVideo from '@site/src/components/global/LoomVideo';

<LoomVideo
	src={`https://www.loom.com/embed/d6ac5431b2cd4286ab1b85fa346f7df9?sid=a9b2ee9f-fa46-49df-bfb3-ee83f87225a5`}
	hideOwner
/>

---

## Broker Network: The First Layer

The first layer of our technology is the Broker Network, where each Node is a Streamr Broker Node that has installed the `logStore` Plugin. This layer is effectively a decentralized version of the Streamr Broker Storage Plugin, whereby Nodes receive real-time events over Streams, store those events into their respective local time-series database, and expose connections over HTTP for query-ability.

Log Store has taken a great effort to embed this layer within the Streamr framework. The developer experience should resemble Streamr’s quite closely. The Log Store Client package shares many of the same methods as the Streamr Client and only complements the Streamr Client with a new `query` method.

## Validator Network: The Second Layer

In order to guarantee the operational performance of the Broker layer, the second Validator layer is required. This Validator layer not only behaves as an authority over the Broker layer — determining which Nodes are rewarded and penalized, and how digital assets should be consumed from developers interacting with the network—but also behaves as the storage mechanism so that Streamr data is uploaded to Arweave in a fully decentralized manner.

The Validator layer builds upon the KYVE Network, a blockchain designed to coordinate decentralized pools of Nodes responsible for producing consenus based data lakes.

## Data Upload and Reporting Process

Once data is pushed to Arweave, a recursive process occurs. Nodes within the Broker layer are responsible for reading from this decentralized data lake and pushing reports to our Polygon Smart Contract. This reporting process takes heavy inspiration from [Chainlink’s Off-Chain Reporting (OCR)](https://docs.chain.link/architecture-overview/off-chain-reporting) strategy to minimize gas fees and improve decentralization.

## Decentralizing Verifiable Custom Data

Verifying the validity data is the process of checking if data has successfully passed through a pre-programmed protocol.

The Log Store Network is a platform for data protocols, whereby systems can program checks on data including whether it is managed by Log Store and therefore tamper-proof and immutable.

An example of this can be demonstrated through a Web3-integrated CPC Advertising Campaign:

1. A Publisher responsible for surfacing an advertisement publishes a signed event when the Advertisement is clicked
2. The Advertiser subscribes to the data stream and publishes a counter-signature when the click yields a page visit - authorising payment for the click
3. Oracle Network
   1. queries the Log Store Network for the most recent set of events & signatures
   2. (_optional_) verifies the integrity of the query response to ensure data was indeed stored and managed in a zero-party manner (_by verifying signatures in to the response's `Consensus` header_)
   3. Yields a data model that a Smart Contract can use to process token re-allocation
4. Smart Contract
   1. verifies the addresses & signature of the Advertiser & Publisher
   2. allocates a portion of Advertiser funds to the Publisher for each authorised event
5. The Publisher can now withdraw their allocated tokens

---

:::note
The full litepaper explaining the network, its consensus mechanisms, and more will release soon.
:::
